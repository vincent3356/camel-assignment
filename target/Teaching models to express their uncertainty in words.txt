Teaching models to express their uncertainty in words
May 28, 2022May 28, 2022
 We show that a GPT-3 model can learn to express uncertainty about its own
answers in natural language -- without use of model logits. When given a
question, the model generates both an answer and a level of confidence (e.g.
"90% confidence" or "high confidence"). These levels map to probabilities that
are well calibrated. The model also remains moderately calibrated under
distribution shift, and is sensitive to uncertainty in its own answers, rather
than imitating human examples. To our knowledge, this is the first time a model
has been shown to express calibrated uncertainty about its own answers in
natural language. For testing calibration, we introduce the CalibratedMath
suite of tasks. We compare the calibration of uncertainty expressed in words
("verbalized probability") to uncertainty extracted from model logits. Both
kinds of uncertainty are capable of generalizing calibration under distribution
shift. We also provide evidence that GPT-3's ability to generalize calibration
depends on pre-trained latent representations that correlate with epistemic
uncertainty over its answers.

Stephanie Lin,Jacob Hilton,Owain Evans,
https://www.amazon.com/Stephanie-Lin/e/B0865Q93XT%3Fref=dbs_a_mng_rwt_scns_share,https://www.slinart.net/,https://www.stephanielinme.com/,https://www.kron4.com/author/stephanie-lin/,https://openreview.net/profile?id=~Stephanie_Lin1
https://digitalcommons.unl.edu/dissertations/AAI3689720/,https://scholar.google.com/citations?user=WyKvz7EAAAAJ&hl=en,https://openreview.net/profile?id=~Jacob_Hilton1,https://www.semanticscholar.org/author/Jacob-Hilton/144890163,https://paperswithcode.com/author/jacob-hilton
https://fromtheashesphx.com/author/owainjohn96/,https://www.gophnx.com/author/owain/,https://www.semanticscholar.org/author/Owain-Evans/47107786,https://scholar.google.com/citations?user=4VpTwzIAAAAJ&hl=en,https://www.lesswrong.com/users/owain_evans